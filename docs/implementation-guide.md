# Simple Agentic RAG - êµ¬í˜„ ê°€ì´ë“œ

## 1. ê°œë°œ í™˜ê²½ ì„¤ì •

### 1.1 í•„ìˆ˜ íŒ¨í‚¤ì§€

```
# requirements.txt
requests>=2.31.0              # Ollama API í˜¸ì¶œ
chromadb>=0.5.0               # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
sentence-transformers>=3.0.0  # ì„ë² ë”© ëª¨ë¸
python-dotenv>=1.0.0          # í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
```

### 1.2 í™˜ê²½ ë³€ìˆ˜

```bash
# .env.example
OLLAMA_URL=http://localhost:11434       # Ollama ì„œë²„ ì£¼ì†Œ
LLM_MODEL=gemma3:12b                   # ì‚¬ìš©í•  LLM ëª¨ë¸
MCP_CONFIG_PATH=mcp_config.json        # MCP ì„œë²„ ì„¤ì • ê²½ë¡œ
CHROMA_PERSIST_DIR=./data/chroma       # ChromaDB ì €ì¥ ê²½ë¡œ
EMBEDDING_MODEL=BAAI/bge-m3       # ì„ë² ë”© ëª¨ë¸ëª…
HITL_MODE=auto                         # HITL ëª¨ë“œ (auto/strict/off)
```

### 1.3 ì‚¬ì „ ì¤€ë¹„

```bash
# Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.com/install.sh | sh
ollama pull gemma3:12b
```

---

## 2. Phase 1 êµ¬í˜„: ë„¤ì´í‹°ë¸Œ Tool Calling

### 2.1 ì „ì²´ íë¦„

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant Agent as Agent Core
    participant LLM as Ollama (gemma3:12b)
    participant MCP as MCP Client
    participant Server as MCP Server

    User->>Agent: ì§ˆë¬¸ ì…ë ¥
    Agent->>LLM: ì§ˆë¬¸ + MCP ë„êµ¬ ëª©ë¡ ì „ì†¡

    alt LLMì´ ë„êµ¬ í˜¸ì¶œì„ ê²°ì •í•œ ê²½ìš°
        LLM-->>Agent: tool_calls ì‘ë‹µ (search_vector_db)
        Agent->>MCP: call_tool("vector-search__search_vector_db", args)
        MCP->>Server: JSON-RPC tools/call
        Server-->>MCP: ê²€ìƒ‰ ê²°ê³¼
        MCP-->>Agent: ê²°ê³¼ ë°˜í™˜
        Agent->>LLM: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì „ì†¡
        LLM-->>Agent: ìµœì¢… í…ìŠ¤íŠ¸ ë‹µë³€
    else LLMì´ ì§ì ‘ ë‹µë³€í•˜ëŠ” ê²½ìš°
        LLM-->>Agent: í…ìŠ¤íŠ¸ ë‹µë³€ (ë„êµ¬ ë¯¸ì‚¬ìš©)
    end

    Agent-->>User: ìµœì¢… ë‹µë³€ ì „ë‹¬
```

### 2.2 í•µì‹¬ ì½”ë“œ êµ¬ì¡° - `agent.py`

```python
"""
Agent Core - Ollama + MCP ê¸°ë°˜ Tool Calling ë£¨í”„

í•µì‹¬ ë¡œì§:
1. ì‚¬ìš©ì ë©”ì‹œì§€ + MCP ë„êµ¬ ëª©ë¡ì„ Ollamaì—ê²Œ ì „ì†¡
2. ì‘ë‹µì— tool_callsê°€ ìˆìœ¼ë©´ MCPë¥¼ í†µí•´ ë„êµ¬ ì‹¤í–‰
3. ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ Ollamaì—ê²Œ ì „ì†¡ (ë°˜ë³µ)
4. text ì‘ë‹µì´ ë‚˜ì˜¤ë©´ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ë°˜í™˜
"""

from llm_adapter import OllamaAdapter
from mcp_client import MCPClient

MAX_TOOL_CALLS = 5  # ë¬´í•œ ë£¨í”„ ë°©ì§€

class AgentCore:
    def __init__(self, llm: OllamaAdapter, mcp: MCPClient, system_prompt: str):
        self.llm = llm
        self.mcp = mcp
        self.system_prompt = system_prompt

    def run(self, messages: list) -> str:
        tools = self.mcp.get_tools_for_llm()
        full_messages = [{"role": "system", "content": self.system_prompt}] + messages

        for _ in range(MAX_TOOL_CALLS):
            response = self.llm.chat(full_messages, tools=tools)

            if not response.has_tool_calls():
                return response.content

            # ë„êµ¬ í˜¸ì¶œ â†’ MCPë¥¼ í†µí•´ ì‹¤í–‰ â†’ ê²°ê³¼ ìˆ˜ì§‘
            full_messages.append({
                "role": "assistant",
                "content": response.content,
                "tool_calls": [{"id": tc.id, "type": "function",
                    "function": {"name": tc.name, "arguments": tc.arguments}}
                    for tc in response.tool_calls]
            })

            for tc in response.tool_calls:
                result = self.mcp.call_tool(tc.name, tc.arguments)
                full_messages.append({
                    "role": "tool", "tool_call_id": tc.id, "content": result
                })

        return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
```

### 2.3 MCP ì„œë²„ - `mcp_servers/vector_search_server.py`

ë„êµ¬ëŠ” MCP ì„œë²„ë¡œ ë¶„ë¦¬ë˜ì–´ í”ŒëŸ¬ê·¸ì¸ì²˜ëŸ¼ ë™ì‘í•œë‹¤. ì½”ë“œ ìˆ˜ì • ì—†ì´ `mcp_config.json`ì— ë“±ë¡ë§Œ í•˜ë©´ ëœë‹¤.

```python
"""Vector Search MCP Server - ë²¡í„° DB ê²€ìƒ‰ ë„êµ¬ë¥¼ MCPë¡œ ì œê³µ"""

import json, sys
import chromadb
from sentence_transformers import SentenceTransformer

embedder = SentenceTransformer("BAAI/bge-m3")
chroma = chromadb.PersistentClient(path="./data/chroma")

TOOLS = [{
    "name": "search_vector_db",
    "description": "ì‚¬ë‚´ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    "inputSchema": {
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "ê²€ìƒ‰ ì¿¼ë¦¬"},
            "top_k": {"type": "integer", "default": 3}
        },
        "required": ["query"]
    }
}]

def search(query, top_k=3):
    col = chroma.get_collection("documents")
    results = col.query(
        query_embeddings=[embedder.encode(query).tolist()], n_results=top_k
    )
    docs = [{"content": d, "metadata": results["metadatas"][0][i]}
            for i, d in enumerate(results["documents"][0])]
    return {"content": [{"type": "text", "text": json.dumps(docs, ensure_ascii=False)}]}

if __name__ == "__main__":
    for line in sys.stdin:
        req = json.loads(line.strip())
        m = req.get("method", "")
        if m == "initialize":
            r = {"protocolVersion": "2024-11-05", "capabilities": {"tools": {}},
                 "serverInfo": {"name": "vector-search"}}
        elif m == "tools/list":
            r = {"tools": TOOLS}
        elif m == "tools/call":
            p = req["params"]
            r = search(p["arguments"]["query"], p["arguments"].get("top_k", 3))
        else:
            r = {}
        sys.stdout.write(json.dumps({"jsonrpc": "2.0", "id": req.get("id"), "result": r}) + "\n")
        sys.stdout.flush()
```

### 2.4 ë„êµ¬ ì„¤ì • - `mcp_config.json`

```json
{
  "mcpServers": {
    "vector-search": {
      "command": "python",
      "args": ["src/mcp_servers/vector_search_server.py"]
    },
    "web-search": {
      "command": "python",
      "args": ["src/mcp_servers/web_search_server.py"]
    }
  }
}
```

ë„êµ¬ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ ì´ íŒŒì¼ì— ì„œë²„ë§Œ ë“±ë¡í•˜ë©´ ëœë‹¤ (ì½”ë“œ ë³€ê²½ 0ì¤„).

---

## 3. Phase 2 êµ¬í˜„: Router íŒ¨í„´

### 3.1 ë¼ìš°íŒ… íë¦„

```mermaid
flowchart TD
    Input["ì‚¬ìš©ì ì§ˆë¬¸"]

    Input --> RouterCall["Router LLM í˜¸ì¶œ<br/>(Structured Output)"]

    RouterCall --> Parse["ì‘ë‹µ íŒŒì‹±"]

    Parse --> Switch{"route ê°’ í™•ì¸"}

    Switch -->|"INTERNAL_SEARCH"| Internal["Agent Core í˜¸ì¶œ<br/>(search_vector_db ë„êµ¬ë§Œ í™œì„±í™”)"]
    Switch -->|"WEB_SEARCH"| Web["Agent Core í˜¸ì¶œ<br/>(web_search ë„êµ¬ë§Œ í™œì„±í™”)"]
    Switch -->|"CHITCHAT"| Chat["LLM ì§ì ‘ ì‘ë‹µ<br/>(ë„êµ¬ ì—†ì´)"]
    Switch -->|"íŒŒì‹± ì‹¤íŒ¨"| Fallback["ê¸°ë³¸ê°’: INTERNAL_SEARCH"]

    Internal --> Response["ìµœì¢… ì‘ë‹µ"]
    Web --> Response
    Chat --> Response
    Fallback --> Internal

    style RouterCall fill:#2196F3,color:#fff
    style Switch fill:#FF9800,color:#fff
```

### 3.2 Router êµ¬í˜„ - `router.py`

```python
"""
Router - ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ê¸°

ê²½ëŸ‰ LLM í˜¸ì¶œë¡œ ì§ˆë¬¸ì„ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•œë‹¤.
"""

from llm_adapter import OllamaAdapter
from prompts.router import ROUTER_PROMPT

class Router:
    VALID_ROUTES = {"INTERNAL_SEARCH", "WEB_SEARCH", "CHITCHAT"}

    def __init__(self, llm: OllamaAdapter):
        self.llm = llm

    def classify(self, query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì—¬ ë¼ìš°íŒ… ê²½ë¡œë¥¼ ë°˜í™˜í•œë‹¤."""

        response = self.llm.chat(messages=[
            {"role": "system", "content": ROUTER_PROMPT},
            {"role": "user", "content": query}
        ])

        route = response.content.strip().upper()

        if route not in self.VALID_ROUTES:
            return "INTERNAL_SEARCH"  # í´ë°±

        return route
```

### 3.3 Router í”„ë¡¬í”„íŠ¸ - `prompts/router.py`

```python
ROUTER_PROMPT = """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì½ê³  ì•„ë˜ ì„¸ ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”.

## ì¹´í…Œê³ ë¦¬

- INTERNAL_SEARCH: ì‚¬ë‚´ ë¬¸ì„œ, ì •ì±…, ê°€ì´ë“œë¼ì¸, ì—…ë¬´ ì ˆì°¨, íšŒì‚¬ ê´€ë ¨ ì§ˆë¬¸
- WEB_SEARCH: ìµœì‹  ë‰´ìŠ¤, ì‹¤ì‹œê°„ ë°ì´í„°, ì™¸ë¶€ ê¸°ìˆ  ì •ë³´, ë‚ ì”¨, ì£¼ê°€ ë“±
- CHITCHAT: ì¼ë°˜ ì¸ì‚¬, ì¡ë‹´, í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ ì§€ì‹ ë“± ê²€ìƒ‰ì´ í•„ìš” ì—†ëŠ” ì§ˆë¬¸

## ê·œì¹™

1. ë°˜ë“œì‹œ ìœ„ ì„¸ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. ì–´ë–¤ ì„¤ëª…ë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
3. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ INTERNAL_SEARCHë¥¼ ì„ íƒí•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹

ì¹´í…Œê³ ë¦¬_ì´ë¦„"""
```

---

## 4. Phase 3 êµ¬í˜„: ë‹¨ì¼ í”¼ë“œë°± ë£¨í”„

### 4.1 CRAG ê°„ì†Œí™” ìƒíƒœ ë¨¸ì‹ 

```mermaid
stateDiagram-v2
    [*] --> Retrieve: ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ 

    Retrieve --> Grade: ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬

    Grade --> Generate: PASS (ë¬¸ì„œ ê´€ë ¨ì„± ì¶©ë¶„)
    Grade --> Rewrite: FAIL (ë¬¸ì„œ ê´€ë ¨ì„± ë¶€ì¡±)

    Rewrite --> Retrieve: ì¬ì‘ì„±ëœ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰<br/>(ìµœëŒ€ 1íšŒ)

    Rewrite --> FallbackGenerate: ì¬ì‹œë„ ì´ˆê³¼ ì‹œ

    Generate --> [*]: ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ë°˜í™˜
    FallbackGenerate --> [*]: í´ë°± ë‹µë³€ ë°˜í™˜

    note right of Grade
        LLMì´ ê²€ìƒ‰ ê²°ê³¼ì˜
        ê´€ë ¨ì„±ì„ PASS/FAILë¡œ
        ì´ì§„ íŒë‹¨
    end note

    note right of Rewrite
        ì›ë³¸ ì§ˆë¬¸ì„ LLMì´
        ë” ë‚˜ì€ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ
        ì¬ì‘ì„±
    end note
```

### 4.2 Grader êµ¬í˜„ - `grader.py`

```python
"""
Grader - ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± í‰ê°€ê¸°

ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì¶©ë¶„í•œì§€ ì´ì§„ íŒë‹¨(PASS/FAIL)ì„ ìˆ˜í–‰í•œë‹¤.
"""

from llm_adapter import OllamaAdapter
from prompts.grader import GRADER_PROMPT

class Grader:
    def __init__(self, llm: OllamaAdapter):
        self.llm = llm

    def evaluate(self, query: str, documents: list[dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•œë‹¤."""

        docs_text = "\n\n---\n\n".join(
            f"[ë¬¸ì„œ {i+1}]\n{doc['content']}"
            for i, doc in enumerate(documents)
        )

        response = self.llm.chat(messages=[
            {"role": "system", "content": GRADER_PROMPT},
            {
                "role": "user",
                "content": (
                    f"## ì‚¬ìš©ì ì§ˆë¬¸\n{query}\n\n"
                    f"## ê²€ìƒ‰ëœ ë¬¸ì„œ\n{docs_text}"
                )
            }
        ])

        result = response.content.strip().upper()
        return result if result in ("PASS", "FAIL") else "PASS"
```

### 4.3 Query Rewriter êµ¬í˜„

```python
"""
Rewriter - ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„±ê¸°

Graderì—ì„œ FAIL íŒì •ì„ ë°›ì€ ê²½ìš°, ì›ë³¸ ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜í•œë‹¤.
"""

from llm_adapter import OllamaAdapter
from prompts.rewriter import REWRITER_PROMPT

class QueryRewriter:
    def __init__(self, llm: OllamaAdapter):
        self.llm = llm

    def rewrite(self, original_query: str) -> str:
        """ì›ë³¸ ì§ˆë¬¸ì„ ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„±í•œë‹¤."""

        response = self.llm.chat(messages=[
            {"role": "system", "content": REWRITER_PROMPT},
            {"role": "user", "content": f"ì›ë³¸ ì§ˆë¬¸: {original_query}"}
        ])

        return response.content.strip()
```

### 4.4 Grader í”„ë¡¬í”„íŠ¸ - `prompts/grader.py`

```python
GRADER_PROMPT = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ìì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¹„êµí•˜ì—¬ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.

## íŒë‹¨ ê¸°ì¤€

PASS: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ í•˜ë‚˜ ì´ìƒì´ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë‚˜ ì£¼ì œì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ í¬í•¨
FAIL: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ì „í˜€ ë¬´ê´€í•˜ê±°ë‚˜ í•„ìš”í•œ ì •ë³´ê°€ ì „í˜€ ì—†ìŒ

## ê·œì¹™

1. PASS ë˜ëŠ” FAIL ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ê´€ë ¨ ìˆìœ¼ë©´ PASSì…ë‹ˆë‹¤.
3. í™•ì‹ ì´ ì—†ìœ¼ë©´ PASSë¥¼ ì„ íƒí•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹

PASS ë˜ëŠ” FAIL"""
```

### 4.5 Rewriter í”„ë¡¬í”„íŠ¸ - `prompts/rewriter.py`

```python
REWRITER_PROMPT = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì— ë” ì í•©í•œ í˜•íƒœë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

## ì¬ì‘ì„± ê·œì¹™

1. í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ëª…í™•í•˜ê²Œ í‘œí˜„
2. ë¶ˆí•„ìš”í•œ ì¡°ì‚¬, ì–´ë¯¸ë¥¼ ì œê±°
3. ë™ì˜ì–´ë‚˜ ê´€ë ¨ ìš©ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
4. ì›ë³¸ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
5. ì¬ì‘ì„±ëœ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´)

## ì˜ˆì‹œ

ì›ë³¸: "íšŒì‚¬ì—ì„œ ì—°ì°¨ ì“°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•´?"
ì¬ì‘ì„±: "ì—°ì°¨ íœ´ê°€ ì‹ ì²­ ì ˆì°¨ ë°©ë²• ê°€ì´ë“œ"

ì›ë³¸: "ìƒˆë¡œ ì…ì‚¬í–ˆëŠ”ë° ë­ë¶€í„° í•´ì•¼ í•˜ì§€?"
ì¬ì‘ì„±: "ì‹ ê·œ ì…ì‚¬ì ì˜¨ë³´ë”© ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ˆì°¨"

## ì¶œë ¥ í˜•ì‹

ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ (í•œ ì¤„)"""
```

---

## 5. ì „ì²´ í†µí•©: `main.py`

### 5.1 í†µí•© ì‹¤í–‰ íë¦„ (Phase 1~4 ì „ì²´)

```mermaid
flowchart TD
    Start["main.py ì‹¤í–‰"]

    Start --> Init["ì´ˆê¸°í™”<br/>- Agent/Router/Planner/Grader/HITL ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"]

    Init --> InputLoop["ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°<br/>(while True)"]

    InputLoop --> GetQuery["ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°"]

    GetQuery --> Route["Phase 2: Router.classify(query)"]

    Route --> Switch{"ë¼ìš°íŒ… ê²°ê³¼"}

    Switch -->|"CHITCHAT"| DirectLLM["LLM ì§ì ‘ ë‹µë³€"]
    Switch -->|"INTERNAL_SEARCH<br/>WEB_SEARCH"| Plan["Phase 2.5: Planner.plan(query)"]

    Plan --> Search["Phase 1: Agent.search(plan.search_queries)"]

    Search --> GradeCheck["Phase 3: Grader.evaluate(query, docs)"]

    GradeCheck --> GradeResult{"í‰ê°€ ê²°ê³¼"}

    GradeResult -->|"PASS"| CalcConf["ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"]
    GradeResult -->|"FAIL"| Rewrite["QueryRewriter.rewrite(query)"]

    Rewrite --> RetrySearch["ì¬ê²€ìƒ‰ (1íšŒ)"]
    RetrySearch --> CalcConf

    CalcConf --> HITL["Phase 4: HITL.request_review(context)"]

    HITL -->|"ìŠ¹ì¸"| Output["ë‹µë³€ ì¶œë ¥"]
    HITL -->|"ìˆ˜ì •"| Output
    HITL -->|"ì¬ê²€ìƒ‰"| Search
    HITL -->|"ê±°ë¶€"| RejectMsg["ê±°ë¶€ ë©”ì‹œì§€ ì¶œë ¥"]

    DirectLLM --> Output
    RejectMsg --> Feedback

    Output --> Feedback["HITL #4: ì‚¬í›„ í”¼ë“œë°± ìˆ˜ì§‘ (ğŸ‘/ğŸ‘)"]

    Feedback --> InputLoop

    style Start fill:#4CAF50,color:#fff
    style Route fill:#2196F3,color:#fff
    style Plan fill:#E91E63,color:#fff
    style GradeCheck fill:#FF9800,color:#fff
    style HITL fill:#9C27B0,color:#fff
    style Output fill:#607D8B,color:#fff
```

### 5.2 í†µí•© ì½”ë“œ ìŠ¤ì¼€ì¹˜

```python
"""
main.py - Simple Agentic RAG ì§„ì…ì 

Phase 1~4ë¥¼ ëª¨ë‘ í†µí•©í•œ ìµœì¢… ì‹¤í–‰ íŒŒì¼ì´ë‹¤.
- Phase 1: ë„¤ì´í‹°ë¸Œ Tool Calling (Ollama + MCP)
- Phase 2: Router íŒ¨í„´
- Phase 2.5: Query Planner
- Phase 3: ë‹¨ì¼ í”¼ë“œë°± ë£¨í”„ (CRAG)
- Phase 4: Human in the Loop
"""

from llm_adapter import OllamaAdapter
from mcp_client import MCPClient
from agent import AgentCore
from router import Router
from planner import QueryPlanner
from grader import Grader, QueryRewriter
from hitl import HITLManager, HITLContext, FeedbackStore
from prompts.system import SYSTEM_PROMPT
from config import Config

def main():
    config = Config()

    # í•µì‹¬ ì¸í”„ë¼ ì´ˆê¸°í™”
    llm = OllamaAdapter(model=config.llm_model, base_url=config.ollama_url)
    mcp = MCPClient(config_path=config.mcp_config_path)
    mcp.connect_all()

    # íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ëª¨ë‘ ê°™ì€ LLM ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
    agent = AgentCore(llm=llm, mcp=mcp, system_prompt=SYSTEM_PROMPT)
    router = Router(llm=llm)
    planner = QueryPlanner(llm=llm)
    grader = Grader(llm=llm)
    rewriter = QueryRewriter(llm=llm)
    hitl = HITLManager(mode=config.hitl_mode)  # "auto" / "strict" / "off"
    feedback_store = FeedbackStore()

    conversation_history = []

    print("Simple Agentic RAG Bot (ì¢…ë£Œ: quit)")

    while True:
        query = input("\n[ì‚¬ìš©ì] ").strip()
        if query.lower() in ("quit", "exit", "ì¢…ë£Œ"):
            break

        # Phase 2: ë¼ìš°íŒ…
        route = router.classify(query)
        print(f"  [ë¼ìš°íŒ…] {route}")

        if route == "CHITCHAT":
            answer = agent.direct_answer(query, conversation_history)
            print(f"\n[ë´‡] {answer}")
        else:
            # Phase 2.5: ì§ˆì˜ ë¶„ì„ & ìµœì í™”
            plan = planner.plan(query, route, conversation_history)
            print(f"  [í”Œë˜ë‹] ì˜ë„: {plan.intent}")
            print(f"  [í”Œë˜ë‹] ê²€ìƒ‰ì–´: {plan.search_queries}")

            tool_filter = (
                "search_vector_db" if route == "INTERNAL_SEARCH"
                else "web_search"
            )

            # Phase 1: Tool Calling ê¸°ë°˜ ê²€ìƒ‰ (ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©)
            if plan.is_multi():
                all_documents = []
                for sq in plan.search_queries:
                    _, docs = agent.search_and_answer(
                        sq, conversation_history, tool_filter
                    )
                    all_documents.extend(docs)
                # ì¤‘ë³µ ì œê±°
                seen = set()
                documents = []
                for doc in all_documents:
                    key = doc["content"][:100]
                    if key not in seen:
                        seen.add(key)
                        documents.append(doc)
                answer = agent.generate_from_docs(query, documents)
            else:
                answer, documents = agent.search_and_answer(
                    plan.search_queries[0], conversation_history, tool_filter
                )

            # Phase 3: ê²€ìƒ‰ ê²°ê³¼ í‰ê°€
            retry_count = 0
            if documents:
                grade = grader.evaluate(query, documents)
                print(f"  [í‰ê°€] {grade}")

                if grade == "FAIL":
                    rewritten = rewriter.rewrite(query)
                    print(f"  [ì¬ì‘ì„±] {rewritten}")
                    answer, documents = agent.search_and_answer(
                        rewritten, conversation_history, tool_filter
                    )
                    retry_count = 1
                    grade = "PASS"  # ì¬ê²€ìƒ‰ í›„ ê°•ì œ ì§„í–‰

            # Phase 4: Human in the Loop
            confidence = hitl.calculator.calculate(
                grader_result=grade if documents else "PASS",
                vector_scores=[d.get("distance", 0) for d in documents],
                retry_count=retry_count
            )

            context = HITLContext(
                query=query, answer=answer,
                confidence=confidence, documents=documents,
                route=route, search_queries=plan.search_queries
            )

            decision = hitl.request_review(context)

            if decision.action == "approve":
                final_answer = answer
            elif decision.action == "edit":
                final_answer = decision.edited_answer
            elif decision.action == "retry":
                final_answer, _ = agent.search_and_answer(
                    decision.new_query, conversation_history, tool_filter
                )
            elif decision.action == "reject":
                final_answer = "ë‹µë³€ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            else:
                final_answer = answer

            print(f"\n[ë´‡] {final_answer}")
            answer = final_answer

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ìµœê·¼ 10í„´)
        conversation_history.append({"role": "user", "content": query})
        conversation_history.append({"role": "assistant", "content": answer})
        conversation_history = conversation_history[-20:]  # 10í„´ = 20ê°œ ë©”ì‹œì§€

        # ì‚¬í›„ í”¼ë“œë°± ìˆ˜ì§‘
        feedback = hitl.collect_feedback(query, answer)
        if feedback:
            feedback_store.save(feedback)

    # MCP ì„œë²„ ì¢…ë£Œ
    mcp.disconnect_all()

if __name__ == "__main__":
    main()
```

---

## 6. ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ êµ¬í˜„ - `vectorstore/ingest.py`

### 6.1 ì¸ì œìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

```mermaid
flowchart LR
    A["data/documents/"] --> B["íŒŒì¼ íƒìƒ‰<br/>(glob)"]
    B --> C["íŒŒì¼ ì½ê¸°<br/>(ë¡œë”)"]
    C --> D["í…ìŠ¤íŠ¸ ë¶„í• <br/>(500ì ì²­í¬)"]
    D --> E["ì„ë² ë”© ìƒì„±<br/>(BAAI/bge-m3)"]
    E --> F["ChromaDB ì €ì¥"]

    style A fill:#607D8B,color:#fff
    style D fill:#2196F3,color:#fff
    style E fill:#FF9800,color:#fff
    style F fill:#9C27B0,color:#fff
```

### 6.2 êµ¬í˜„ ì½”ë“œ

```python
"""
ingest.py - ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

data/documents/ í´ë”ì˜ íŒŒì¼ì„ ì½ì–´ ChromaDBì— ë²¡í„°ë¡œ ì €ì¥í•œë‹¤.
"""

import os
import glob
import chromadb
from sentence_transformers import SentenceTransformer

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

def chunk_text(text: str) -> list[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ê³ ì • í¬ê¸° ì²­í¬ë¡œ ë¶„í• í•œë‹¤."""
    chunks = []
    start = 0
    while start < len(text):
        end = start + CHUNK_SIZE
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - CHUNK_OVERLAP
    return chunks

def ingest_documents(docs_dir: str = "./data/documents"):
    """ë¬¸ì„œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ë²¡í„° DBì— ì¸ì œìŠ¤íŠ¸í•œë‹¤."""

    embedder = SentenceTransformer("BAAI/bge-m3")
    client = chromadb.PersistentClient(path="./data/chroma")

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±
    try:
        client.delete_collection("documents")
    except ValueError:
        pass

    collection = client.create_collection(
        name="documents",
        metadata={"hnsw:space": "cosine"}
    )

    all_chunks = []
    all_metadatas = []
    all_ids = []

    # ì§€ì› í™•ì¥ì
    extensions = ["*.txt", "*.md", "*.pdf"]

    for ext in extensions:
        for filepath in glob.glob(os.path.join(docs_dir, "**", ext), recursive=True):
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read()

            chunks = chunk_text(text)
            filename = os.path.basename(filepath)

            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadatas.append({
                    "source": filename,
                    "chunk_index": i
                })
                all_ids.append(f"{filename}_{i}")

    if not all_chunks:
        print("ì¸ì œìŠ¤íŠ¸í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°°ì¹˜ ì„ë² ë”© ë° ì €ì¥
    embeddings = embedder.encode(all_chunks).tolist()

    collection.add(
        documents=all_chunks,
        embeddings=embeddings,
        metadatas=all_metadatas,
        ids=all_ids
    )

    print(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ë¥¼ ì¸ì œìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    ingest_documents()
```

---

## 7. í…ŒìŠ¤íŠ¸ ì „ëµ

### 7.1 í…ŒìŠ¤íŠ¸ êµ¬ì¡°

```mermaid
graph TD
    subgraph "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Tests)"
        UT1["test_router.py<br/>ë¼ìš°í„° ë¶„ë¥˜ ì •í™•ë„"]
        UT2["test_planner.py<br/>ì¿¼ë¦¬ í”Œë˜ë„ˆ ìµœì í™”"]
        UT3["test_grader.py<br/>í‰ê°€ê¸° íŒë‹¨ ì •í™•ë„"]
        UT4["test_tools.py<br/>ë„êµ¬ ì‹¤í–‰ ì •ìƒ ë™ì‘"]
        UT5["test_hitl.py<br/>HITL ì‹ ë¢°ë„ ê³„ì‚°"]
    end

    subgraph "í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests)"
        IT1["test_agent.py<br/>ì—ì´ì „íŠ¸ ì „ì²´ ë£¨í”„"]
        IT2["test_ingest.py<br/>ì¸ì œìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸"]
        IT3["test_feedback.py<br/>í”¼ë“œë°± ì €ì¥/ì¡°íšŒ"]
    end

    subgraph "E2E í…ŒìŠ¤íŠ¸"
        E2E["test_e2e.py<br/>ì§ˆë¬¸â†’ë‹µë³€ ì „ì²´ íë¦„"]
    end

    UT1 & UT2 & UT3 & UT4 & UT5 --> IT1
    IT1 & IT2 & IT3 --> E2E

    style UT1 fill:#4CAF50,color:#fff
    style UT2 fill:#4CAF50,color:#fff
    style UT3 fill:#4CAF50,color:#fff
    style UT4 fill:#4CAF50,color:#fff
    style UT5 fill:#4CAF50,color:#fff
    style IT1 fill:#2196F3,color:#fff
    style IT2 fill:#2196F3,color:#fff
    style IT3 fill:#2196F3,color:#fff
    style E2E fill:#FF9800,color:#fff
```

### 7.2 í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì˜ˆì‹œ

| í…ŒìŠ¤íŠ¸ | ì…ë ¥ | ê¸°ëŒ€ ê²°ê³¼ |
|--------|------|-----------|
| Router - ì‚¬ë‚´ ë¬¸ì„œ | "íœ´ê°€ ì‹ ì²­ ë°©ë²• ì•Œë ¤ì¤˜" | `INTERNAL_SEARCH` |
| Router - ì›¹ ê²€ìƒ‰ | "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?" | `WEB_SEARCH` |
| Router - ì¡ë‹´ | "ì•ˆë…•í•˜ì„¸ìš”!" | `CHITCHAT` |
| Planner - ë§¥ë½ í•´ì†Œ | "ê·¸ê±° ë‹¤ì‹œ ì•Œë ¤ì¤˜" (ì´ì „: íœ´ê°€) | ì¿¼ë¦¬ì— "íœ´ê°€" í¬í•¨ |
| Planner - ë³µí•© ì§ˆë¬¸ | "íœ´ê°€ ê·œì •ì´ë‘ ì¶œì¥ë¹„" | `strategy: MULTI`, ì¿¼ë¦¬ 2ê°œ |
| Planner - ì¿¼ë¦¬ ìµœì í™” | "ì–´ë–»ê²Œ í•˜ë©´ ë¼?" | ëª…ì‚¬êµ¬ ì¤‘ì‹¬ ì¿¼ë¦¬ ë³€í™˜ |
| Grader - ê´€ë ¨ ë¬¸ì„œ | ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì œê³µ | `PASS` |
| Grader - ë¬´ê´€ ë¬¸ì„œ | ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ë¬¸ì„œ ì œê³µ | `FAIL` |
| Rewriter | "íšŒì‚¬ì—ì„œ ì—°ì°¨ ì“°ë ¤ë©´?" | í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì¿¼ë¦¬ |
| HITL - HIGH ì‹ ë¢°ë„ | confidence=0.9 | `should_intervene() == "none"` |
| HITL - LOW ì‹ ë¢°ë„ | confidence=0.3 | `should_intervene() == "hard"` |
| HITL - ì‹ ë¢°ë„ ê³„ì‚° | PASS, ìœ ì‚¬ë„ 0.8, ì¬ì‹œë„ 0 | confidence >= 0.7 |
| Agent - ë„êµ¬ í˜¸ì¶œ | ì‚¬ë‚´ ë¬¸ì„œ ì§ˆë¬¸ | `search_vector_db` í˜¸ì¶œë¨ |
| Agent - ì§ì ‘ ë‹µë³€ | "1+1ì€?" | ë„êµ¬ ë¯¸í˜¸ì¶œ, ì§ì ‘ ë‹µë³€ |
| Feedback - ì €ì¥ | ê¸ì • í”¼ë“œë°± | JSONLì— ì •ìƒ ê¸°ë¡ |

---

## 8. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ë„¤ì´í‹°ë¸Œ Tool Calling (Ollama + MCP)

- [ ] í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • (`requirements.txt`, `.env`, `.gitignore`)
- [ ] Ollama ì„¤ì¹˜ ë° `gemma3:12b` ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] `config.py` - ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
- [ ] `llm_adapter.py` - OllamaAdapter (LLM ì¶”ìƒí™”)
- [ ] `mcp_client.py` - MCP í´ë¼ì´ì–¸íŠ¸
- [ ] `mcp_servers/vector_search_server.py` - ë²¡í„° ê²€ìƒ‰ MCP ì„œë²„
- [ ] `mcp_servers/web_search_server.py` - ì›¹ ê²€ìƒ‰ MCP ì„œë²„
- [ ] `mcp_config.json` - MCP ì„œë²„ ì„¤ì •
- [ ] `agent.py` - ì—ì´ì „íŠ¸ ì½”ì–´ (Ollama + MCP Tool Calling ë£¨í”„)
- [ ] `vectorstore/ingest.py` - ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
- [ ] `main.py` - ê¸°ë³¸ CLI ì¸í„°í˜ì´ìŠ¤
- [ ] Phase 1 í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼

### Phase 2: Router íŒ¨í„´

- [ ] `prompts/router.py` - ë¼ìš°í„° í”„ë¡¬í”„íŠ¸
- [ ] `router.py` - ë¼ìš°í„° êµ¬í˜„
- [ ] `main.py` ì— ë¼ìš°í„° í†µí•©
- [ ] Phase 2 í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼

### Phase 2.5: Query Planner

- [ ] `prompts/planner.py` - í”Œë˜ë„ˆ í”„ë¡¬í”„íŠ¸
- [ ] `planner.py` - Query Planner êµ¬í˜„ (ë§¥ë½ í•´ì†Œ, ì¿¼ë¦¬ ìµœì í™”, ë³µí•© ì§ˆë¬¸ ë¶„í•´)
- [ ] `main.py` ì— í”Œë˜ë„ˆ í†µí•© (SINGLE/MULTI ì „ëµ ì²˜ë¦¬)
- [ ] Phase 2.5 í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼

### Phase 3: ë‹¨ì¼ í”¼ë“œë°± ë£¨í”„

- [ ] `prompts/grader.py` - í‰ê°€ í”„ë¡¬í”„íŠ¸
- [ ] `prompts/rewriter.py` - ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
- [ ] `grader.py` - í‰ê°€ê¸° ë° ì¬ì‘ì„±ê¸° êµ¬í˜„
- [ ] `main.py` ì— í”¼ë“œë°± ë£¨í”„ í†µí•©
- [ ] Phase 3 í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼

### Phase 4: Human in the Loop

- [ ] `hitl.py` - HITL ê´€ë¦¬ì, ì‹ ë¢°ë„ ê³„ì‚°ê¸°, í”¼ë“œë°± ìˆ˜ì§‘ê¸°
- [ ] `hitl.py` - FeedbackStore (JSONL ì €ì¥/ì¡°íšŒ)
- [ ] `config.py` ì— HITL ëª¨ë“œ ì„¤ì • ì¶”ê°€ (`auto`/`strict`/`off`)
- [ ] `main.py` ì— HITL í†µí•© (ì‹ ë¢°ë„ ê³„ì‚° â†’ ê²€í†  ìš”ì²­ â†’ í”¼ë“œë°± ìˆ˜ì§‘)
- [ ] Phase 4 í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼
- [ ] E2E í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼ (ì „ì²´ Phase 1~4 í†µí•©)
